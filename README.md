# LLM_D2
GPT-2 scale LLM (150M params), trained from scratch on "The Pile" dataset, achieving 28.5% score in Hallaswag
